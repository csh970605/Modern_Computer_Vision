{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"SWXXpSmUrd24"},"source":["![](https://github.com/rajeevratan84/ModernComputerVision/raw/main/logo_MCV_W.png)\n","\n","# **Replicating LeNet in Tensorflow2.0 using Keras**\n","\n","---\n","\n","In this lesson, we use **Keras with a TensorFlow 2.0** Backend to to replicate both **LeNet** in Keras and train it to **recognize handwritten digits in the MNIST dataset and the 10 images classes of CIFAR10**\n","1. Replicate the LeNet CNN Architecture "]},{"cell_type":"markdown","metadata":{"id":"DlFbhDyYrgTp"},"source":["## **Let's construct LeNet in Keras!**\n","\n","![](https://www.researchgate.net/profile/Sheraz_Khan8/publication/321586653/figure/fig4/AS:568546847014912@1512563539828/The-LeNet-5-Architecture-a-convolutional-neural-network.png)\n","## **LeNet Architecture**\n","S.No | Layers | Output Shape (Height, Width, Channels)\n","--- | --- | ---\n","1 | Input Layer | 32 x 32 x 1\n","2 | Conv2d [6 Filters of size = 5x5, stride = 1, padding = 0 ] | 28 x 28 x 6\n","3 | Average Pooling [stride = 2, padding = 0] | 14 x 14 x 6\n","4 | Conv2d [16 Filters of size = 5x5, stride = 1, padding = 0 ] | 10 x 10 x 16\n","5 | Average Pooling [stride = 2, padding = 0] | 5 x 5 x 16\n","6 | Conv2d [120 Filters of size = 5x5, stride = 1, padding = 0 ] | 1 x 1 x 120\n","7 | Linear1 Layer | 120 \n","8 | Linear2 Layer | 84 \n","9 | Final Linear Layer | 10\n","\n","\n","### **Loading and preprocessing our Data**"]},{"cell_type":"code","metadata":{"id":"4Cr4L1LjRqVk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686228918710,"user_tz":-540,"elapsed":4394,"user":{"displayName":"최세훈","userId":"00148754616075165078"}},"outputId":"1bed255a-b2fc-4301-bb2f-3b7e26db68a0"},"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras.datasets import mnist\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.optimizers import Adadelta\n","\n","(x_train, y_train), (x_test, y_test)  = mnist.load_data()\n","\n","img_rows = x_train[0].shape[0]\n","img_cols = x_train[1].shape[0]\n","\n","x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n","x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n","\n","input_shape = (img_rows, img_cols, 1)\n","\n","x_train = x_train.astype('float32')\n","x_test = x_test.astype('float32')\n","\n","x_train /= 255\n","x_test /= 255\n","\n","# Now we one hot encode outputs\n","y_train = to_categorical(y_train)\n","y_test = to_categorical(y_test)\n","\n","num_classes = y_test.shape[1]\n","num_pixels = x_train.shape[1] * x_train.shape[2]"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11490434/11490434 [==============================] - 1s 0us/step\n"]}]},{"cell_type":"markdown","metadata":{"id":"ywUXRWhKtMCZ"},"source":["### **Now let's create our layers to replicate LeNet**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QeVx6xALslPK","executionInfo":{"status":"ok","timestamp":1686228923124,"user_tz":-540,"elapsed":4416,"user":{"displayName":"최세훈","userId":"00148754616075165078"}},"outputId":"44ef0fd4-321c-46ee-faa2-97d4e86effa0"},"source":["# create model\n","model = Sequential()\n","\n","# 2 sets of CRP (Convolution, RELU, Pooling)\n","model.add(Conv2D(6, (5, 5),  padding = \"same\", input_shape=input_shape))\n","model.add(Activation(\"relu\"))\n","model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2)))\n","\n","model.add(Conv2D(16, (5, 5), padding = \"same\"))\n","model.add(Activation(\"relu\"))\n","model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2)))\n","\n","model.add(Conv2D(120, (5, 5), padding = \"same\"))\n","model.add(Activation(\"relu\"))\n","model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2)))\n","\n","# Fully connected layers (w/ RELU)\n","model.add(Flatten())\n","model.add(Dense(120))\n","model.add(Activation(\"relu\"))\n","\n","model.add(Dense(84))\n","model.add(Activation(\"relu\"))\n","\n","# Softmax (for classification)\n","model.add(Dense(num_classes))\n","model.add(Activation(\"softmax\"))\n","           \n","model.compile(loss='categorical_crossentropy', optimizer = Adadelta(), metrics = ['accuracy'])\n","    \n","print(model.summary())"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 28, 28, 6)         156       \n","                                                                 \n"," activation (Activation)     (None, 28, 28, 6)         0         \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 14, 14, 6)        0         \n"," )                                                               \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 14, 14, 16)        2416      \n","                                                                 \n"," activation_1 (Activation)   (None, 14, 14, 16)        0         \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 7, 7, 16)         0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 7, 7, 120)         48120     \n","                                                                 \n"," activation_2 (Activation)   (None, 7, 7, 120)         0         \n","                                                                 \n"," max_pooling2d_2 (MaxPooling  (None, 3, 3, 120)        0         \n"," 2D)                                                             \n","                                                                 \n"," flatten (Flatten)           (None, 1080)              0         \n","                                                                 \n"," dense (Dense)               (None, 120)               129720    \n","                                                                 \n"," activation_3 (Activation)   (None, 120)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 84)                10164     \n","                                                                 \n"," activation_4 (Activation)   (None, 84)                0         \n","                                                                 \n"," dense_2 (Dense)             (None, 10)                850       \n","                                                                 \n"," activation_5 (Activation)   (None, 10)                0         \n","                                                                 \n","=================================================================\n","Total params: 191,426\n","Trainable params: 191,426\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"]}]},{"cell_type":"markdown","metadata":{"id":"6XEqUat3tQph"},"source":["### **Now let us train LeNet on our MNIST Dataset**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2f2gBg44tOwq","executionInfo":{"status":"ok","timestamp":1686229127471,"user_tz":-540,"elapsed":204350,"user":{"displayName":"최세훈","userId":"00148754616075165078"}},"outputId":"9922c1e9-b443-44ae-e2be-0c4a03d965c3"},"source":["batch_size = 128\n","epochs = 50\n","\n","history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), shuffle=True)\n","\n","model.save(\"mnist_LeNet.h5\")\n","\n","scores = model.evaluate(x_test, y_test, verbose=1)\n","print('Test loss:', scores[0])\n","print('Test accuracy:', scores[1])"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","469/469 [==============================] - 15s 8ms/step - loss: 2.2983 - accuracy: 0.1651 - val_loss: 2.2938 - val_accuracy: 0.2043\n","Epoch 2/50\n","469/469 [==============================] - 3s 5ms/step - loss: 2.2895 - accuracy: 0.2280 - val_loss: 2.2849 - val_accuracy: 0.2493\n","Epoch 3/50\n","469/469 [==============================] - 3s 6ms/step - loss: 2.2804 - accuracy: 0.2592 - val_loss: 2.2756 - val_accuracy: 0.2693\n","Epoch 4/50\n","469/469 [==============================] - 3s 5ms/step - loss: 2.2704 - accuracy: 0.2665 - val_loss: 2.2641 - val_accuracy: 0.2618\n","Epoch 5/50\n","469/469 [==============================] - 3s 5ms/step - loss: 2.2573 - accuracy: 0.2591 - val_loss: 2.2493 - val_accuracy: 0.2532\n","Epoch 6/50\n","469/469 [==============================] - 4s 8ms/step - loss: 2.2414 - accuracy: 0.2570 - val_loss: 2.2311 - val_accuracy: 0.2545\n","Epoch 7/50\n","469/469 [==============================] - 3s 5ms/step - loss: 2.2212 - accuracy: 0.2591 - val_loss: 2.2078 - val_accuracy: 0.2622\n","Epoch 8/50\n","469/469 [==============================] - 2s 5ms/step - loss: 2.1953 - accuracy: 0.2782 - val_loss: 2.1777 - val_accuracy: 0.3017\n","Epoch 9/50\n","469/469 [==============================] - 2s 5ms/step - loss: 2.1615 - accuracy: 0.3230 - val_loss: 2.1382 - val_accuracy: 0.3462\n","Epoch 10/50\n","469/469 [==============================] - 3s 6ms/step - loss: 2.1175 - accuracy: 0.3683 - val_loss: 2.0874 - val_accuracy: 0.3749\n","Epoch 11/50\n","469/469 [==============================] - 3s 6ms/step - loss: 2.0606 - accuracy: 0.4017 - val_loss: 2.0206 - val_accuracy: 0.4128\n","Epoch 12/50\n","469/469 [==============================] - 3s 5ms/step - loss: 1.9858 - accuracy: 0.4392 - val_loss: 1.9355 - val_accuracy: 0.4533\n","Epoch 13/50\n","469/469 [==============================] - 3s 5ms/step - loss: 1.8948 - accuracy: 0.4740 - val_loss: 1.8346 - val_accuracy: 0.4909\n","Epoch 14/50\n","469/469 [==============================] - 3s 6ms/step - loss: 1.7884 - accuracy: 0.5069 - val_loss: 1.7192 - val_accuracy: 0.5294\n","Epoch 15/50\n","469/469 [==============================] - 3s 7ms/step - loss: 1.6707 - accuracy: 0.5482 - val_loss: 1.5952 - val_accuracy: 0.5787\n","Epoch 16/50\n","469/469 [==============================] - 3s 5ms/step - loss: 1.5474 - accuracy: 0.5985 - val_loss: 1.4683 - val_accuracy: 0.6260\n","Epoch 17/50\n","469/469 [==============================] - 3s 5ms/step - loss: 1.4230 - accuracy: 0.6450 - val_loss: 1.3435 - val_accuracy: 0.6755\n","Epoch 18/50\n","469/469 [==============================] - 3s 6ms/step - loss: 1.3023 - accuracy: 0.6881 - val_loss: 1.2245 - val_accuracy: 0.7124\n","Epoch 19/50\n","469/469 [==============================] - 3s 6ms/step - loss: 1.1901 - accuracy: 0.7230 - val_loss: 1.1165 - val_accuracy: 0.7411\n","Epoch 20/50\n","469/469 [==============================] - 3s 6ms/step - loss: 1.0891 - accuracy: 0.7482 - val_loss: 1.0207 - val_accuracy: 0.7643\n","Epoch 21/50\n","469/469 [==============================] - 2s 5ms/step - loss: 0.9999 - accuracy: 0.7679 - val_loss: 0.9382 - val_accuracy: 0.7795\n","Epoch 22/50\n","469/469 [==============================] - 3s 6ms/step - loss: 0.9238 - accuracy: 0.7821 - val_loss: 0.8681 - val_accuracy: 0.7944\n","Epoch 23/50\n","469/469 [==============================] - 3s 5ms/step - loss: 0.8593 - accuracy: 0.7933 - val_loss: 0.8089 - val_accuracy: 0.8059\n","Epoch 24/50\n","469/469 [==============================] - 3s 7ms/step - loss: 0.8045 - accuracy: 0.8029 - val_loss: 0.7587 - val_accuracy: 0.8157\n","Epoch 25/50\n","469/469 [==============================] - 3s 6ms/step - loss: 0.7575 - accuracy: 0.8111 - val_loss: 0.7153 - val_accuracy: 0.8242\n","Epoch 26/50\n","469/469 [==============================] - 3s 6ms/step - loss: 0.7172 - accuracy: 0.8191 - val_loss: 0.6783 - val_accuracy: 0.8321\n","Epoch 27/50\n","469/469 [==============================] - 3s 5ms/step - loss: 0.6818 - accuracy: 0.8262 - val_loss: 0.6455 - val_accuracy: 0.8371\n","Epoch 28/50\n","469/469 [==============================] - 3s 6ms/step - loss: 0.6509 - accuracy: 0.8324 - val_loss: 0.6166 - val_accuracy: 0.8455\n","Epoch 29/50\n","469/469 [==============================] - 3s 7ms/step - loss: 0.6234 - accuracy: 0.8386 - val_loss: 0.5911 - val_accuracy: 0.8513\n","Epoch 30/50\n","469/469 [==============================] - 3s 5ms/step - loss: 0.5987 - accuracy: 0.8442 - val_loss: 0.5679 - val_accuracy: 0.8567\n","Epoch 31/50\n","469/469 [==============================] - 2s 5ms/step - loss: 0.5763 - accuracy: 0.8496 - val_loss: 0.5465 - val_accuracy: 0.8623\n","Epoch 32/50\n","469/469 [==============================] - 3s 5ms/step - loss: 0.5555 - accuracy: 0.8555 - val_loss: 0.5269 - val_accuracy: 0.8667\n","Epoch 33/50\n","469/469 [==============================] - 3s 7ms/step - loss: 0.5367 - accuracy: 0.8594 - val_loss: 0.5095 - val_accuracy: 0.8697\n","Epoch 34/50\n","469/469 [==============================] - 3s 6ms/step - loss: 0.5196 - accuracy: 0.8639 - val_loss: 0.4930 - val_accuracy: 0.8753\n","Epoch 35/50\n","469/469 [==============================] - 2s 5ms/step - loss: 0.5039 - accuracy: 0.8681 - val_loss: 0.4781 - val_accuracy: 0.8772\n","Epoch 36/50\n","469/469 [==============================] - 3s 5ms/step - loss: 0.4893 - accuracy: 0.8715 - val_loss: 0.4647 - val_accuracy: 0.8812\n","Epoch 37/50\n","469/469 [==============================] - 2s 5ms/step - loss: 0.4756 - accuracy: 0.8749 - val_loss: 0.4519 - val_accuracy: 0.8843\n","Epoch 38/50\n","469/469 [==============================] - 3s 7ms/step - loss: 0.4632 - accuracy: 0.8780 - val_loss: 0.4402 - val_accuracy: 0.8869\n","Epoch 39/50\n","469/469 [==============================] - 3s 5ms/step - loss: 0.4514 - accuracy: 0.8806 - val_loss: 0.4291 - val_accuracy: 0.8898\n","Epoch 40/50\n","469/469 [==============================] - 3s 6ms/step - loss: 0.4403 - accuracy: 0.8830 - val_loss: 0.4181 - val_accuracy: 0.8924\n","Epoch 41/50\n","469/469 [==============================] - 2s 5ms/step - loss: 0.4299 - accuracy: 0.8859 - val_loss: 0.4082 - val_accuracy: 0.8944\n","Epoch 42/50\n","469/469 [==============================] - 3s 6ms/step - loss: 0.4202 - accuracy: 0.8887 - val_loss: 0.3989 - val_accuracy: 0.8969\n","Epoch 43/50\n","469/469 [==============================] - 3s 6ms/step - loss: 0.4110 - accuracy: 0.8907 - val_loss: 0.3902 - val_accuracy: 0.8984\n","Epoch 44/50\n","469/469 [==============================] - 2s 5ms/step - loss: 0.4022 - accuracy: 0.8929 - val_loss: 0.3826 - val_accuracy: 0.8995\n","Epoch 45/50\n","469/469 [==============================] - 3s 5ms/step - loss: 0.3939 - accuracy: 0.8951 - val_loss: 0.3742 - val_accuracy: 0.9014\n","Epoch 46/50\n","469/469 [==============================] - 3s 5ms/step - loss: 0.3860 - accuracy: 0.8967 - val_loss: 0.3666 - val_accuracy: 0.9025\n","Epoch 47/50\n","469/469 [==============================] - 3s 6ms/step - loss: 0.3786 - accuracy: 0.8986 - val_loss: 0.3601 - val_accuracy: 0.9040\n","Epoch 48/50\n","469/469 [==============================] - 3s 5ms/step - loss: 0.3714 - accuracy: 0.8998 - val_loss: 0.3526 - val_accuracy: 0.9049\n","Epoch 49/50\n","469/469 [==============================] - 2s 5ms/step - loss: 0.3646 - accuracy: 0.9021 - val_loss: 0.3462 - val_accuracy: 0.9070\n","Epoch 50/50\n","469/469 [==============================] - 2s 5ms/step - loss: 0.3581 - accuracy: 0.9036 - val_loss: 0.3401 - val_accuracy: 0.9084\n","313/313 [==============================] - 1s 3ms/step - loss: 0.3401 - accuracy: 0.9084\n","Test loss: 0.3400520384311676\n","Test accuracy: 0.9083999991416931\n"]}]}]}