{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"SWXXpSmUrd24"},"source":["![](https://github.com/rajeevratan84/ModernComputerVision/raw/main/logo_MCV_W.png)\n","\n","# **Replicating AlexNet in Tensorflow2.0 using Keras**\n","\n","---\n","\n","In this lesson, we use **Keras with a TensorFlow 2.0** Backend to to replicate both **AlexNet** in Keras and train it to **recognize handwritten digits in the MNIST dataset and the 10 images classes of CIFAR10**\n","1. Replicate the AlexNet CNN Architecture "]},{"cell_type":"markdown","metadata":{"id":"b4i3pvXxtf32"},"source":["## **Now let's replicate AlexNET and train in on the CIFAR10 Dataset**\n","\n","AlexNet was the 2012 ImageNet winner achieved a top-5 error of 15.3%, more than 10.8 percentage points lower than that of the runner up!\n","\n","![](https://paperswithcode.com/media/methods/Screen_Shot_2020-06-22_at_6.35.45_PM.png)\n","\n","![](https://production-media.paperswithcode.com/datasets/CIFAR-10-0000000431-b71f61c0_U5n3Glr.jpg)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z_UI-juKTRLa","executionInfo":{"status":"ok","timestamp":1686228812522,"user_tz":-540,"elapsed":20201,"user":{"displayName":"최세훈","userId":"00148754616075165078"}},"outputId":"3c5ddcea-6751-41f0-a280-21c3a43802b3"},"source":["from tensorflow.keras.datasets import cifar10\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras.optimizers import Adadelta\n","from tensorflow.keras.utils import to_categorical\n","\n","# Loads the CIFAR dataset\n","(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","\n","# Display our data shape/dimensions\n","print('x_train shape:', x_train.shape)\n","print(x_train.shape[0], 'train samples')\n","print(x_test.shape[0], 'test samples')\n","\n","# Now we one hot encode outputs\n","num_classes = 10\n","y_train = to_categorical(y_train, num_classes)\n","y_test = to_categorical(y_test, num_classes)"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","170498071/170498071 [==============================] - 14s 0us/step\n","x_train shape: (50000, 32, 32, 3)\n","50000 train samples\n","10000 test samples\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cJQPK5fKvxrn","executionInfo":{"status":"ok","timestamp":1686228815643,"user_tz":-540,"elapsed":3131,"user":{"displayName":"최세훈","userId":"00148754616075165078"}},"outputId":"0b3f9795-b60a-4e52-8814-0b414315e767"},"source":["l2_reg = 0.001\n","\n","model = Sequential()\n","\n","# 1st Conv Layer \n","model.add(Conv2D(96, (11, 11), input_shape=x_train.shape[1:], padding='same', kernel_regularizer=l2(l2_reg)))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","# 2nd Conv Layer \n","model.add(Conv2D(256, (5, 5), padding='same'))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","# 3rd Conv Layer \n","model.add(ZeroPadding2D((1, 1)))\n","model.add(Conv2D(512, (3, 3), padding='same'))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","# 4th Conv Layer \n","model.add(ZeroPadding2D((1, 1)))\n","model.add(Conv2D(1024, (3, 3), padding='same'))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","\n","# 5th Conv Layer \n","model.add(ZeroPadding2D((1, 1)))\n","model.add(Conv2D(1024, (3, 3), padding='same'))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","# 1st FC Layer\n","model.add(Flatten())\n","model.add(Dense(3072))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(Dropout(0.5))\n","\n","# 2nd FC Layer\n","model.add(Dense(4096))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(Dropout(0.5))\n","\n","# 3rd FC Layer\n","model.add(Dense(num_classes))\n","model.add(BatchNormalization())\n","model.add(Activation('softmax'))\n","\n","print(model.summary())\n","\n","model.compile(loss='categorical_crossentropy', optimizer = Adadelta(), metrics = ['accuracy'])\n"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 32, 32, 96)        34944     \n","                                                                 \n"," batch_normalization (BatchN  (None, 32, 32, 96)       384       \n"," ormalization)                                                   \n","                                                                 \n"," activation (Activation)     (None, 32, 32, 96)        0         \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 16, 16, 96)       0         \n"," )                                                               \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 16, 16, 256)       614656    \n","                                                                 \n"," batch_normalization_1 (Batc  (None, 16, 16, 256)      1024      \n"," hNormalization)                                                 \n","                                                                 \n"," activation_1 (Activation)   (None, 16, 16, 256)       0         \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 8, 8, 256)        0         \n"," 2D)                                                             \n","                                                                 \n"," zero_padding2d (ZeroPadding  (None, 10, 10, 256)      0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 10, 10, 512)       1180160   \n","                                                                 \n"," batch_normalization_2 (Batc  (None, 10, 10, 512)      2048      \n"," hNormalization)                                                 \n","                                                                 \n"," activation_2 (Activation)   (None, 10, 10, 512)       0         \n","                                                                 \n"," max_pooling2d_2 (MaxPooling  (None, 5, 5, 512)        0         \n"," 2D)                                                             \n","                                                                 \n"," zero_padding2d_1 (ZeroPaddi  (None, 7, 7, 512)        0         \n"," ng2D)                                                           \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 7, 7, 1024)        4719616   \n","                                                                 \n"," batch_normalization_3 (Batc  (None, 7, 7, 1024)       4096      \n"," hNormalization)                                                 \n","                                                                 \n"," activation_3 (Activation)   (None, 7, 7, 1024)        0         \n","                                                                 \n"," zero_padding2d_2 (ZeroPaddi  (None, 9, 9, 1024)       0         \n"," ng2D)                                                           \n","                                                                 \n"," conv2d_4 (Conv2D)           (None, 9, 9, 1024)        9438208   \n","                                                                 \n"," batch_normalization_4 (Batc  (None, 9, 9, 1024)       4096      \n"," hNormalization)                                                 \n","                                                                 \n"," activation_4 (Activation)   (None, 9, 9, 1024)        0         \n","                                                                 \n"," max_pooling2d_3 (MaxPooling  (None, 4, 4, 1024)       0         \n"," 2D)                                                             \n","                                                                 \n"," flatten (Flatten)           (None, 16384)             0         \n","                                                                 \n"," dense (Dense)               (None, 3072)              50334720  \n","                                                                 \n"," batch_normalization_5 (Batc  (None, 3072)             12288     \n"," hNormalization)                                                 \n","                                                                 \n"," activation_5 (Activation)   (None, 3072)              0         \n","                                                                 \n"," dropout (Dropout)           (None, 3072)              0         \n","                                                                 \n"," dense_1 (Dense)             (None, 4096)              12587008  \n","                                                                 \n"," batch_normalization_6 (Batc  (None, 4096)             16384     \n"," hNormalization)                                                 \n","                                                                 \n"," activation_6 (Activation)   (None, 4096)              0         \n","                                                                 \n"," dropout_1 (Dropout)         (None, 4096)              0         \n","                                                                 \n"," dense_2 (Dense)             (None, 10)                40970     \n","                                                                 \n"," batch_normalization_7 (Batc  (None, 10)               40        \n"," hNormalization)                                                 \n","                                                                 \n"," activation_7 (Activation)   (None, 10)                0         \n","                                                                 \n","=================================================================\n","Total params: 78,990,642\n","Trainable params: 78,970,462\n","Non-trainable params: 20,180\n","_________________________________________________________________\n","None\n"]}]},{"cell_type":"code","metadata":{"id":"qDyixr27vzXF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686230957040,"user_tz":-540,"elapsed":2141399,"user":{"displayName":"최세훈","userId":"00148754616075165078"}},"outputId":"b82b656a-ca5b-43ce-a0e3-a4250f9be582"},"source":["batch_size = 64\n","epochs = 25\n","\n","history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), shuffle=True)\n","\n","model.save(\"CIFAR10_AlexNet_10_Epoch.h5\")\n","\n","scores = model.evaluate(x_test, y_test, verbose=1)\n","print('Test loss:', scores[0])\n","print('Test accuracy:', scores[1])"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/25\n","782/782 [==============================] - 100s 106ms/step - loss: 2.1365 - accuracy: 0.2420 - val_loss: 1.7595 - val_accuracy: 0.3887\n","Epoch 2/25\n","782/782 [==============================] - 83s 106ms/step - loss: 1.8399 - accuracy: 0.3451 - val_loss: 1.6597 - val_accuracy: 0.4339\n","Epoch 3/25\n","782/782 [==============================] - 82s 105ms/step - loss: 1.7423 - accuracy: 0.3866 - val_loss: 1.6001 - val_accuracy: 0.4613\n","Epoch 4/25\n","782/782 [==============================] - 83s 106ms/step - loss: 1.6748 - accuracy: 0.4179 - val_loss: 1.5526 - val_accuracy: 0.4794\n","Epoch 5/25\n","782/782 [==============================] - 83s 106ms/step - loss: 1.6264 - accuracy: 0.4395 - val_loss: 1.5186 - val_accuracy: 0.4941\n","Epoch 6/25\n","782/782 [==============================] - 82s 106ms/step - loss: 1.5877 - accuracy: 0.4554 - val_loss: 1.4909 - val_accuracy: 0.5061\n","Epoch 7/25\n","782/782 [==============================] - 82s 105ms/step - loss: 1.5548 - accuracy: 0.4723 - val_loss: 1.4643 - val_accuracy: 0.5181\n","Epoch 8/25\n","782/782 [==============================] - 83s 106ms/step - loss: 1.5243 - accuracy: 0.4858 - val_loss: 1.4449 - val_accuracy: 0.5290\n","Epoch 9/25\n","782/782 [==============================] - 82s 105ms/step - loss: 1.5008 - accuracy: 0.4989 - val_loss: 1.4251 - val_accuracy: 0.5377\n","Epoch 10/25\n","782/782 [==============================] - 83s 106ms/step - loss: 1.4783 - accuracy: 0.5092 - val_loss: 1.4070 - val_accuracy: 0.5430\n","Epoch 11/25\n","782/782 [==============================] - 83s 106ms/step - loss: 1.4558 - accuracy: 0.5199 - val_loss: 1.3942 - val_accuracy: 0.5486\n","Epoch 12/25\n","782/782 [==============================] - 82s 105ms/step - loss: 1.4349 - accuracy: 0.5307 - val_loss: 1.3818 - val_accuracy: 0.5575\n","Epoch 13/25\n","782/782 [==============================] - 83s 106ms/step - loss: 1.4200 - accuracy: 0.5394 - val_loss: 1.3655 - val_accuracy: 0.5626\n","Epoch 14/25\n","782/782 [==============================] - 83s 106ms/step - loss: 1.4008 - accuracy: 0.5489 - val_loss: 1.3561 - val_accuracy: 0.5659\n","Epoch 15/25\n","782/782 [==============================] - 83s 106ms/step - loss: 1.3845 - accuracy: 0.5565 - val_loss: 1.3473 - val_accuracy: 0.5695\n","Epoch 16/25\n","782/782 [==============================] - 82s 105ms/step - loss: 1.3675 - accuracy: 0.5630 - val_loss: 1.3344 - val_accuracy: 0.5744\n","Epoch 17/25\n","782/782 [==============================] - 83s 106ms/step - loss: 1.3528 - accuracy: 0.5709 - val_loss: 1.3274 - val_accuracy: 0.5795\n","Epoch 18/25\n","782/782 [==============================] - 83s 106ms/step - loss: 1.3370 - accuracy: 0.5781 - val_loss: 1.3188 - val_accuracy: 0.5852\n","Epoch 19/25\n","782/782 [==============================] - 82s 105ms/step - loss: 1.3229 - accuracy: 0.5867 - val_loss: 1.3068 - val_accuracy: 0.5912\n","Epoch 20/25\n","782/782 [==============================] - 83s 106ms/step - loss: 1.3095 - accuracy: 0.5942 - val_loss: 1.2995 - val_accuracy: 0.5933\n","Epoch 21/25\n","782/782 [==============================] - 83s 106ms/step - loss: 1.2932 - accuracy: 0.6041 - val_loss: 1.2966 - val_accuracy: 0.5967\n","Epoch 22/25\n","782/782 [==============================] - 82s 105ms/step - loss: 1.2836 - accuracy: 0.6084 - val_loss: 1.2853 - val_accuracy: 0.6006\n","Epoch 23/25\n","782/782 [==============================] - 82s 106ms/step - loss: 1.2703 - accuracy: 0.6137 - val_loss: 1.2794 - val_accuracy: 0.6034\n","Epoch 24/25\n","782/782 [==============================] - 83s 106ms/step - loss: 1.2548 - accuracy: 0.6240 - val_loss: 1.2739 - val_accuracy: 0.6045\n","Epoch 25/25\n","782/782 [==============================] - 83s 106ms/step - loss: 1.2452 - accuracy: 0.6274 - val_loss: 1.2677 - val_accuracy: 0.6068\n","313/313 [==============================] - 7s 19ms/step - loss: 1.2677 - accuracy: 0.6067\n","Test loss: 1.2677406072616577\n","Test accuracy: 0.6067000031471252\n"]}]}]}