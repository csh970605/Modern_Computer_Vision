{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"omWff1IYp_y2"},"source":["![](https://github.com/rajeevratan84/ModernComputerVision/raw/main/logo_MCV_W.png)\n","# **YOLOv3 in using cv2.dnn.readNetFrom()**\n","\n","####**In this lesson we'll learn how to load a pre-trained YOLOV3 Model and use OpenCV to run inferences over a few images**\n"]},{"cell_type":"code","source":["!wget https://moderncomputervision.s3.eu-west-2.amazonaws.com/YOLO.zip\n","!unzip -qq YOLO.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lW3P401BEC-l","executionInfo":{"status":"ok","timestamp":1685626016258,"user_tz":-540,"elapsed":12332,"user":{"displayName":"최세훈","userId":"00148754616075165078"}},"outputId":"fc192da8-cb5f-4e00-c379-944ee5e8b390"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-06-01 13:26:45--  https://moderncomputervision.s3.eu-west-2.amazonaws.com/YOLO.zip\n","Resolving moderncomputervision.s3.eu-west-2.amazonaws.com (moderncomputervision.s3.eu-west-2.amazonaws.com)... 3.5.244.142\n","Connecting to moderncomputervision.s3.eu-west-2.amazonaws.com (moderncomputervision.s3.eu-west-2.amazonaws.com)|3.5.244.142|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 231559154 (221M) [application/zip]\n","Saving to: ‘YOLO.zip’\n","\n","YOLO.zip            100%[===================>] 220.83M  36.0MB/s    in 6.7s    \n","\n","2023-06-01 13:26:52 (33.2 MB/s) - ‘YOLO.zip’ saved [231559154/231559154]\n","\n"]}]},{"cell_type":"code","metadata":{"id":"4BlZva9GdJVQ","executionInfo":{"status":"ok","timestamp":1685626016718,"user_tz":-540,"elapsed":461,"user":{"displayName":"최세훈","userId":"00148754616075165078"}}},"source":["import numpy as np\n","import time\n","import cv2\n","import os\n","from os import listdir\n","from os.path import isfile, join\n","from matplotlib import pyplot as plt \n","\n","def imshow(title = \"Image\", image = None, size = 10):\n","    h, w = image.shape[:2]\n","    aspect_ratio = w/h\n","    plt.figure(figsize=(size * aspect_ratio,size))\n","    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n","    plt.axis('off')\n","    plt.title(title)\n","    plt.show()"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qhQD1v84dqxu"},"source":["## **YOLO Object Detection**\n","\n","![](https://github.com/csh970605/Machine-LearningA-Z/assets/28240052/2710b848-27ec-453b-b417-048ccf2c697f)\n","\n","**Steps Invovled**\n","\n","1. Use Pretrained YOLOV3 weights (237MB)- https://pjreddie.com/media/files/yolov3.weights\n","2. Create our blob object which is our loaded model\n","3. Set the backend that runs the model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lb-FtCJRdOG8","executionInfo":{"status":"ok","timestamp":1685626017751,"user_tz":-540,"elapsed":1034,"user":{"displayName":"최세훈","userId":"00148754616075165078"}},"outputId":"69014580-dbe0-4197-e3f0-ac9fad334711"},"source":["labelsPath = \"YOLO/yolo/coco.names\"\n","LABELS = open(labelsPath).read().strip().split(\"\\n\")\n"," \n","# We now need to initialize a list of colors to represent each possible class label\n","COLORS = np.random.randint(0, 255, size=(len(LABELS), 3), dtype=\"uint8\")\n","\n","print(\"Loading YOLO weights...\") \n","\n","weights_path = \"YOLO/yolo/yolov3.weights\" \n","cfg_path = \"YOLO/yolo/yolov3.cfg\"\n","\n","# Create our blob object\n","net = cv2.dnn.readNetFromDarknet(cfg_path, weights_path)\n","\n","# Set our backend\n","net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n","\n","print(\"Our YOLO Layers\")\n","model = net.getLayerNames()\n","\n","# There are 254 Layers\n","print(len(model), model)"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading YOLO weights...\n","Our YOLO Layers\n","254 ('conv_0', 'bn_0', 'leaky_1', 'conv_1', 'bn_1', 'leaky_2', 'conv_2', 'bn_2', 'leaky_3', 'conv_3', 'bn_3', 'leaky_4', 'shortcut_4', 'conv_5', 'bn_5', 'leaky_6', 'conv_6', 'bn_6', 'leaky_7', 'conv_7', 'bn_7', 'leaky_8', 'shortcut_8', 'conv_9', 'bn_9', 'leaky_10', 'conv_10', 'bn_10', 'leaky_11', 'shortcut_11', 'conv_12', 'bn_12', 'leaky_13', 'conv_13', 'bn_13', 'leaky_14', 'conv_14', 'bn_14', 'leaky_15', 'shortcut_15', 'conv_16', 'bn_16', 'leaky_17', 'conv_17', 'bn_17', 'leaky_18', 'shortcut_18', 'conv_19', 'bn_19', 'leaky_20', 'conv_20', 'bn_20', 'leaky_21', 'shortcut_21', 'conv_22', 'bn_22', 'leaky_23', 'conv_23', 'bn_23', 'leaky_24', 'shortcut_24', 'conv_25', 'bn_25', 'leaky_26', 'conv_26', 'bn_26', 'leaky_27', 'shortcut_27', 'conv_28', 'bn_28', 'leaky_29', 'conv_29', 'bn_29', 'leaky_30', 'shortcut_30', 'conv_31', 'bn_31', 'leaky_32', 'conv_32', 'bn_32', 'leaky_33', 'shortcut_33', 'conv_34', 'bn_34', 'leaky_35', 'conv_35', 'bn_35', 'leaky_36', 'shortcut_36', 'conv_37', 'bn_37', 'leaky_38', 'conv_38', 'bn_38', 'leaky_39', 'conv_39', 'bn_39', 'leaky_40', 'shortcut_40', 'conv_41', 'bn_41', 'leaky_42', 'conv_42', 'bn_42', 'leaky_43', 'shortcut_43', 'conv_44', 'bn_44', 'leaky_45', 'conv_45', 'bn_45', 'leaky_46', 'shortcut_46', 'conv_47', 'bn_47', 'leaky_48', 'conv_48', 'bn_48', 'leaky_49', 'shortcut_49', 'conv_50', 'bn_50', 'leaky_51', 'conv_51', 'bn_51', 'leaky_52', 'shortcut_52', 'conv_53', 'bn_53', 'leaky_54', 'conv_54', 'bn_54', 'leaky_55', 'shortcut_55', 'conv_56', 'bn_56', 'leaky_57', 'conv_57', 'bn_57', 'leaky_58', 'shortcut_58', 'conv_59', 'bn_59', 'leaky_60', 'conv_60', 'bn_60', 'leaky_61', 'shortcut_61', 'conv_62', 'bn_62', 'leaky_63', 'conv_63', 'bn_63', 'leaky_64', 'conv_64', 'bn_64', 'leaky_65', 'shortcut_65', 'conv_66', 'bn_66', 'leaky_67', 'conv_67', 'bn_67', 'leaky_68', 'shortcut_68', 'conv_69', 'bn_69', 'leaky_70', 'conv_70', 'bn_70', 'leaky_71', 'shortcut_71', 'conv_72', 'bn_72', 'leaky_73', 'conv_73', 'bn_73', 'leaky_74', 'shortcut_74', 'conv_75', 'bn_75', 'leaky_76', 'conv_76', 'bn_76', 'leaky_77', 'conv_77', 'bn_77', 'leaky_78', 'conv_78', 'bn_78', 'leaky_79', 'conv_79', 'bn_79', 'leaky_80', 'conv_80', 'bn_80', 'leaky_81', 'conv_81', 'permute_82', 'yolo_82', 'identity_83', 'conv_84', 'bn_84', 'leaky_85', 'upsample_85', 'concat_86', 'conv_87', 'bn_87', 'leaky_88', 'conv_88', 'bn_88', 'leaky_89', 'conv_89', 'bn_89', 'leaky_90', 'conv_90', 'bn_90', 'leaky_91', 'conv_91', 'bn_91', 'leaky_92', 'conv_92', 'bn_92', 'leaky_93', 'conv_93', 'permute_94', 'yolo_94', 'identity_95', 'conv_96', 'bn_96', 'leaky_97', 'upsample_97', 'concat_98', 'conv_99', 'bn_99', 'leaky_100', 'conv_100', 'bn_100', 'leaky_101', 'conv_101', 'bn_101', 'leaky_102', 'conv_102', 'bn_102', 'leaky_103', 'conv_103', 'bn_103', 'leaky_104', 'conv_104', 'bn_104', 'leaky_105', 'conv_105', 'permute_106', 'yolo_106')\n"]}]},{"cell_type":"markdown","metadata":{"id":"S8cfxbOwejXq"},"source":["The input to the network is a called blob object. \n","\n","The function ```cv.dnn.blobFromImage(img, scale, size, mean)``` transforms the image into a blob:\n","\n","```blob = cv.dnn.blobFromImage(img, 1/255.0, (416, 416), swapRB=True, crop=False)```\n","\n","**It has the following parameters:**\n","\n","1. the image to transform\n","2. the scale factor (1/255 to scale the pixel values to [0..1])\n","3. the size, here a 416x416 square image\n","4. the mean value (default=0)\n","5. the option swapBR=True (since OpenCV uses BGR)\n","\n","**Note** A blob is a 4D numpy array object (images, channels, width, height). The image below shows the red channel of the blob. You notice the brightness of the red jacket in the background.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1fC5TDxVkfMgeacI7RWC2pWyCXEGje-53"},"id":"PtC5_CRLeVV8","executionInfo":{"status":"ok","timestamp":1685626041876,"user_tz":-540,"elapsed":24128,"user":{"displayName":"최세훈","userId":"00148754616075165078"}},"outputId":"7d22dbd9-9003-46d3-aae6-da30dec890d9"},"source":["print(\"Starting Detections...\")\n","mypath = \"YOLO/images/\"\n","file_names = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n","\n","# Loop through images run them through our classifer\n","for file in file_names:\n","  \n","    image = cv2.imread(mypath+file)\n","    (H, W) = image.shape[:2]\n"," \n","    # we want only the output layer names that we need from YOLO\n","    model = net.getLayerNames()\n","    # We get the predicted output from unconnected out layers which is 82, 94, 106 because of the image above.\n","    model = [model[i - 1] for i in net.getUnconnectedOutLayers()]\n","\n","    # Now we contruct our blob from our input images\n","    blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n","    # We set our input to our image blob\n","    net.setInput(blob)\n","    # Then we run a forward pass through the network\n","    layerOutputs = net.forward(model)\n","\n","    # we initialize our lists for our detected bounding boxes, confidences, and classes\n","    boxes = []\n","    confidences = []\n","    IDs = []\n","\n","    # Loop over each of the layer outputs\n","    for output in layerOutputs:\n","\n","        # Loop over each detection\n","        for detection in output:\n","            # Obtain class ID and probality of detection\n","            scores = detection[5:]\n","            classID = np.argmax(scores)\n","            confidence = scores[classID]\n","\n","            # We keep only the most probably predictions\n","            if confidence > 0.75:\n","                # We scale the bounding box coordinates relative to the image\n","                # Note: YOLO actually returns the center (x, y) of the bounding\n","                # box followed by the width and height of the box\n","                box = detection[0:4] * np.array([W, H, W, H])\n","                (centerX, centerY, width, height) = box.astype(\"int\")\n","\n","                # Get the the top and left corner of the bounding box\n","                # Remember we alredy have the width and height\n","                x = int(centerX - (width / 2))\n","                y = int(centerY - (height / 2))\n","\n","                # Append our list of bounding box coordinates, confidences and class IDs\n","                boxes.append([x, y, int(width), int(height)])\n","                confidences.append(float(confidence))\n","                IDs.append(classID)\n","\n","    # Now we apply non-maxima suppression to reduce overlapping bounding boxes\n","    idxs = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.3)\n","\n","    # We proceed once a detection has been found\n","    if len(idxs) > 0:\n","        # iterate over the indexes we are keeping\n","        for i in idxs.flatten():\n","            # Get the bounding box coordinates\n","            (x, y) = (boxes[i][0], boxes[i][1])\n","            (w, h) = (boxes[i][2], boxes[i][3])\n","\n","            # Draw our bounding boxes and put our class label on the image\n","            color = [int(c) for c in COLORS[IDs[i]]]\n","            cv2.rectangle(image, (x, y), (x + w, y + h), color, 3)\n","            text = \"{}: {:.4f}\".format(LABELS[IDs[i]], confidences[i])\n","            cv2.putText(image, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n","\n","    # show the output image\n","    imshow(\"YOLO Detections\", image, size = 12)"],"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"AZjRzsrLotwA"},"source":["## **NOTE:** **How to Perform non maximum suppression given boxes and corresponding scores.**\n","\n","```indices\t=\tcv.dnn.NMSBoxes(\tbboxes, scores, score_threshold, nms_threshold[, eta[, top_k]]```\n","\n","\n","\n","**Parameters**\n","- bboxes\ta set of bounding boxes to apply NMS.\n","- scores\ta set of corresponding confidences.\n","- score_threshold\ta threshold used to filter boxes by score.\n","- nms_threshold\ta threshold used in non maximum suppression.\n","indices\tthe kept indices of bboxes after NMS.\n","- eta\ta coefficient in adaptive threshold formula: nms_thresholdi+1=eta⋅nms_thresholdi.\n","- top_k\tif >0, keep at most top_k picked indices.\n"]}]}